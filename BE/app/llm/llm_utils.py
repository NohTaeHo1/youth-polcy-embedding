# import requests
# import time
# import requests

# def summarize_with_ollama(prompt: str, model: str = "mistral", temperature: float = 0.7, max_tokens: int = 512) -> str:
#     try:
#         response = requests.post(
#             url="http://localhost:11434/api/generate",
#             json={
#                 "model": model,
#                 "prompt": prompt,
#                 "temperature": temperature,
#                 "max_tokens": max_tokens,
#                 "stream": False
#             },
#             timeout=600
#         )
#         response.raise_for_status()
#         result = response.json()
#         return result.get("response", "").strip()
#     except Exception as e:
#         print(f"[LLM Error] {e}")
#         return ""

# def wait_for_ollama_ready(timeout=180):
#     print("ğŸŸ¡ Ollama ì¤€ë¹„ ìƒíƒœ í™•ì¸ ì¤‘...")
#     start = time.time()
#     while time.time() - start < timeout:
#         try:
#             r = requests.get("http://localhost:11434/api/tags", timeout=5)
#             if r.status_code == 200 and "models" in r.json() and r.json()["models"]:
#                 print("ğŸŸ¢ Ollama ì¤€ë¹„ ì™„ë£Œ.")
#                 return True
#         except Exception as e:
#             print(f"ì˜ˆì™¸ ë°œìƒ: {e}")
#         time.sleep(3)
#     raise RuntimeError("ğŸ”´ Ollama ì„œë²„ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


import asyncio
import requests
import json
import aiohttp
from app.config import API_URL,MODEL_NAME
from collections import defaultdict

class llm_utils:
    """
    LLM ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
    ì´ í´ë˜ìŠ¤ëŠ” LLM ìš”ì²­ì„ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ë°˜í™˜í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µ
    ì†ì„±:
    
    """
    def __init__(self):
        self.api_url = API_URL
        self.model = MODEL_NAME
        self.temperature = 0.2

    async def send_request(self, prompt: str) -> str:
        payload = {
            "model": self.model,
            "prompt": prompt,
            "temperature": self.temperature,
        }
        # aiohttp ClientSessionì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸° HTTP ìš”ì²­ ìˆ˜í–‰
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(self.api_url, json=payload, timeout=15) as response:
                    response.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
                    full_response = await response.text()  # ì‘ë‹µì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì½ê¸°
            except aiohttp.ClientError as e:
                print(f"HTTP ìš”ì²­ ì‹¤íŒ¨: {e}")
                raise RuntimeError(f"Ollama API ìš”ì²­ ì‹¤íŒ¨: {e}") from e

        # ì „ì²´ ì‘ë‹µì„ ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³  JSON íŒŒì‹±
        lines = full_response.splitlines()
        all_text = ""
        for line in lines:
            try:
                json_line = json.loads(line.strip())
                all_text += json_line.get("response", "")
            except json.JSONDecodeError as e:
                print(f"JSON decode error: {e}")
                continue

        return all_text.strip() if all_text else "Empty response received"
